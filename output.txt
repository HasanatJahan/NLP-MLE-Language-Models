Answer to Question No.1
The number of unique words in the training corpus is 41739

Answer to Question No.2
The number of tokens in the training corpus is 2568210

Answer to Question No.3
The percentage of words unseen in test is 3.6028823058446755
The percentage of tokens unseen in test is 1.603346113628442

Answer to Question No.4
Percentage of unique bigrams in test not in training is 25.327695560253698
Percentage of bigram tokens in test not in training is 21.704586493318885

Answer to Question No.5
['<s>', 'i', 'look', 'forward', 'to', 'hearing', 'your', 'reply', '.', '</s>']
1. Unigram Log Probability -90.25609082365422
1. Unigram Average Log Probability -9.025609082365422
2. Bigram Model Evaluation -0.0
As the bigram model log probability evaluation is zero, there is no average log probability. Perplexity is undefined
3.Bigram Add One Log Probability -97.13956016607362
3.Bigram Add One Average Log Probability -9.713956016607362

Answer to Question No 6
Perplexity of sentence under unigram model 521.1695852332095
Perplexity of sentence under add one bigram model 839.83145676326

Answer to Question No.7
The unigram log probability for the test corpus is -28124.445292806824
The unigram average log probability for the test corpus is -9.802873925690772
Perplexity of test corpus under unigram model 893.2213504247355
Bigram Model Evaluation on test corpus -0.0
As the bigram model log probability evaluation is zero, there is no average log probability. Perplexity is undefined
Add One Smoothing Bigram Log Probability -31250.970622838657
Add One Smoothing Bigram Average Log Probability -10.892635281574993
Add One Smoothing Bigram Perplexity 1901.1218424221317
